

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="记录对这个框架的基本理解，一些重要的API和一些工程实践。">
  <meta name="author" content="Luty">
  <meta name="keywords" content="">
  
  <title>TensorFlow快进到入坟 | Luty.tech</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/monokai.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"luty.tech","root":"/","version":"1.8.11","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Luty's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/blackhole.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="TensorFlow快进到入坟">
              
                TensorFlow快进到入坟
              
            </span>

            
              <div class="mt-3">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-author" aria-hidden="true"></i>
      Luty
    </span>
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2018-05-25 19:29" pubdate>
        2018年5月25日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      56
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">TensorFlow快进到入坟</h1>
            
            <div class="markdown-body">
              <p>记录对这个框架的基本理解，一些重要的API和一些工程实践。</p>
<span id="more"></span>
<h2 id="三个核心："><a href="#三个核心：" class="headerlink" title="三个核心："></a>三个核心：</h2><ol>
<li>所有的数据都是张量</li>
<li>所有计算都是计算图</li>
<li>执行运算和管理资源都要使用会话</li>
</ol>
<h2 id="一些基本运行原理："><a href="#一些基本运行原理：" class="headerlink" title="一些基本运行原理："></a>一些基本运行原理：</h2><p>Tensorflow的所有计算都在计算图（Graph）中运行，通过集合（Collection）管理资源（变量、张量、队列资源），其中TensorFlow维护了一个tf.GraphKeys.VARIABLES的集合，该集合包含了所有的变量。</p>
<p>例如<code>a = tf.constan([1.0,2.0],name=&quot;a&quot;)</code>中，<code>tf.constant</code>是一个计算，计算的结果是一个<code>Tensor</code>，每一个<code>Tensor</code>都有三个属性：</p>
<ol>
<li>名字（name），作为张量的唯一标识符。</li>
<li>维度（shape），描述张量的维度信息，例如<code>shape=(3,3,1)</code>表示该张量有三个维度（可以理解为多维数组），各个维度长度分别为<code>3</code>,<code>3</code>,<code>1</code>，采用<code>None</code>则表示该维度不定长。</li>
<li>类型（type），数据类型，同其他编程语言。</li>
</ol>
<p>logits:  一个事件发生与该事件不发生的比值的对数</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><div class="hljs code-wrapper"><pre><code class="hljs python">numpy.squeeze(a, axis=<span class="hljs-literal">None</span>)

squeeze()函数的功能是：从矩阵shape中，去掉维度为<span class="hljs-number">1</span>的。例如一个矩阵是的shape是（<span class="hljs-number">5</span>， <span class="hljs-number">1</span>），使用过这个函数后，结果为（<span class="hljs-number">5</span>，）。

参数： 
a是输入的矩阵 
axis : 选择shape中的一维条目的子集。如果在shape大于<span class="hljs-number">1</span>的情况下设置axis，则会引发错误。

tf.gfile.Glob(filename)
Defined <span class="hljs-keyword">in</span> tensorflow/python/lib/io/file_io.py.

Returns a <span class="hljs-built_in">list</span> of files that match the given pattern(s).
   


    
图：一些Tensor和Operation的集合
    
实际上，Tensorflow而是首先将 python 代码所描绘的图转换（即“序列化”）成 Protocol Buffer，再通过 C/C++/CUDA 运行 Protocol Buffer 所定义的图。</code></pre></div>
<h2 id="基础又重要的内容"><a href="#基础又重要的内容" class="headerlink" title="基础又重要的内容"></a>基础又重要的内容</h2><h3 id="1-Part-1"><a href="#1-Part-1" class="headerlink" title="1. Part 1"></a>1. Part 1</h3><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">#计算图和会话的基本使用</span>
g1 = tf.graph()
<span class="hljs-keyword">with</span> g1.as_default():
    <span class="hljs-keyword">pass</span>
	<span class="hljs-comment">#your operation</span>

g2 = tf.graph()
<span class="hljs-keyword">with</span> g1.as_default():
    <span class="hljs-keyword">pass</span>
	<span class="hljs-comment">#your operation</span>

<span class="hljs-keyword">with</span> tf.Session(graph=g1) <span class="hljs-keyword">as</span> sess:
    tf.initializer_all_variables().run()
    <span class="hljs-comment">#your operation</span>

    
<span class="hljs-comment">#定义一个常向量</span>
a = tf.constant([<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],name=<span class="hljs-string">&quot;a&quot;</span>)

<span class="hljs-comment">#定义矩阵乘法</span>
x1 = tf.matmul(x,w1)

<span class="hljs-comment">#定义一个一维变向量，初始化为0, 计算图中张量的初始化需要使用`initializer`</span>
v = tf.get_variable(name=<span class="hljs-string">&quot;v&quot;</span>,initializer=tf.zeros_initializer(shape=[<span class="hljs-number">1</span>]))

<span class="hljs-comment">#定义一个随机矩阵变量，标准差为2，注意get_variable不会处理命名冲突，Varaible则会，因此在需要共享变量名的场景下应采用get_variable</span>
a = tf.Variable(tf.random_normal([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],stddev=<span class="hljs-number">2</span>))

<span class="hljs-comment">#assign用于给变量赋值</span>
tf.assign(w1,w2)

<span class="hljs-comment">#定义了一个placeholder数据入口</span>
x = tf.placeholder(<span class="hljs-built_in">type</span>=tf.float32, shape=(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>), name=<span class="hljs-string">&quot;x-input&quot;</span>)

<span class="hljs-comment">#定义了反向传播算法和梯度下降优化器</span>
train_step = tf.train.GradientDescentOptimizer(learning_rate).minimizer(cross_entropy)</code></pre></div>
<h3 id="2-Part-2"><a href="#2-Part-2" class="headerlink" title="2. Part 2"></a>2. Part 2</h3><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">#tf.reduce_mean 函数用于计算张量tensor某个维度上的的平均值，tf.clip_by_value可以吧数据限制在一个范围之内</span>

<span class="hljs-comment">#例中利用_y张量计算了平均交叉熵。</span>
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,<span class="hljs-number">1e-10</span>,<span class="hljs-number">1.0</span>)))

<span class="hljs-comment">#定义了一个使用softmax回归的交叉熵损失函数</span>
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y,y_)

<span class="hljs-comment">#定义了一个均方差的损失函数</span>
mse = tf.reduce_mean(tf.square(_y-y))

<span class="hljs-comment">#tf.reduce_sum压缩求和，tf.select当参数一条件为真时，会使用参数二的值，否则会使用参数三的值，tf.greater会逐个比较张量中每个元素的大小</span>
<span class="hljs-comment">#以下代码实现了一个简单的自定义损失函数</span>
loss = tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b)

<span class="hljs-comment">#利用tf.contrib.layers.12_regularizer()对原均方差损失函数进行了正则化，lambda表示正则化权重，w为正则化损失参数</span>
loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers<span class="hljs-number">.12</span>_regularizer(<span class="hljs-keyword">lambda</span>)(w)
                     
<span class="hljs-comment">#以下代码中利用add_yo_colection将正则化损失项加入到losses集合中</span>
 tf.add_to_collection(<span class="hljs-string">&#x27;losses&#x27;</span>,tf.contrib.layers<span class="hljs-number">.12</span>_regularizer(<span class="hljs-keyword">lambda</span>)(w))
                     
<span class="hljs-comment">#以下代码定义了一个滑动平均类ema，并定义了滑动平均操作maintain_averages_op，其中decay_rate衰减率，越接近1模型越稳定，step是控制衰减率的变量。</span>
ema = tf.train.ExponentialMovingAverage(decay_rate,step)
maintain_averages_op = ema.apply([v1])</code></pre></div>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="1-简单前馈神经网络"><a href="#1-简单前馈神经网络" class="headerlink" title="1. 简单前馈神经网络"></a>1. 简单前馈神经网络</h3><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -*-coding:utf-8 -*-</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomState

batch_size = <span class="hljs-number">8</span>
w1= tf.Variable(tf.random_normal([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], stddev=<span class="hljs-number">1</span>, seed=<span class="hljs-number">1</span>))
w2= tf.Variable(tf.random_normal([<span class="hljs-number">3</span>, <span class="hljs-number">1</span>], stddev=<span class="hljs-number">1</span>, seed=<span class="hljs-number">1</span>))
x = tf.placeholder(tf.float32, shape=(<span class="hljs-literal">None</span>, <span class="hljs-number">2</span>), name=<span class="hljs-string">&quot;x-input&quot;</span>)
y_= tf.placeholder(tf.float32, shape=(<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>), name=<span class="hljs-string">&#x27;y-input&#x27;</span>)

a = tf.matmul(x, w1)
y = tf.matmul(a, w2)
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="hljs-number">1e-10</span>, <span class="hljs-number">1.0</span>))) 
train_step = tf.train.AdamOptimizer(<span class="hljs-number">0.001</span>).minimize(cross_entropy)

rdm = RandomState(<span class="hljs-number">1</span>)
X = rdm.rand(<span class="hljs-number">128</span>,<span class="hljs-number">2</span>)
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="hljs-number">1e-10</span>, <span class="hljs-number">1.0</span>))) 
train_step = tf.train.AdamOptimizer(<span class="hljs-number">0.001</span>).minimize(cross_entropy)
Y = [[<span class="hljs-built_in">int</span>(x1+x2 &lt; <span class="hljs-number">1</span>)] <span class="hljs-keyword">for</span> (x1, x2) <span class="hljs-keyword">in</span> X]

<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)    
    
    STEPS = <span class="hljs-number">5000</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(STEPS):
        start = (i*batch_size) % <span class="hljs-number">128</span>
        end = (i*batch_size) % <span class="hljs-number">128</span> + batch_size
        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y[start:end]&#125;)
        <span class="hljs-keyword">if</span> i % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:
            total_cross_entropy = sess.run(cross_entropy, feed_dict=&#123;x: X, y_: Y&#125;)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After %d training step(s), cross entropy on all data is %g&quot;</span> % (i, total_cross_entropy))
    
    <span class="hljs-comment"># 输出训练后的参数取值。</span>
    <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;\n&quot;</span>
    <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;w1:&quot;</span>, sess.run(w1)
    <span class="hljs-built_in">print</span> <span class="hljs-string">&quot;w2:&quot;</span>, sess.run(w2)</code></pre></div>
<h3 id="2-MNIST手写数据集-CNN版"><a href="#2-MNIST手写数据集-CNN版" class="headerlink" title="2. MNIST手写数据集 - CNN版"></a>2. MNIST手写数据集 - CNN版</h3><p><strong>定义前向传播流程</strong></p>
<div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -*-coding:utf-8-*-</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

INPUT_NODE = <span class="hljs-number">784</span>    <span class="hljs-comment">#输入的节点数 28*28</span>
OUTPUT_NODE = <span class="hljs-number">10</span>    <span class="hljs-comment">#输出的节点数 10</span>

IMAGE_SIZE = <span class="hljs-number">28</span>     <span class="hljs-comment">#图像大小</span>
NUM_CHANNELS = <span class="hljs-number">1</span>    <span class="hljs-comment">#图像深度</span>
NUM_LABELS = <span class="hljs-number">10</span>     <span class="hljs-comment">#数字种类</span>

CONV1_DEEP = <span class="hljs-number">32</span>     <span class="hljs-comment">#第一层卷积层深度</span>
CONV1_SIZE = <span class="hljs-number">5</span>      <span class="hljs-comment">#第一层卷积层尺寸</span>

CONV2_DEEP = <span class="hljs-number">64</span>     <span class="hljs-comment">#第二层卷积层深度</span>
CONV2_SIZE = <span class="hljs-number">5</span>      <span class="hljs-comment">#第二层卷积层尺寸</span>

FC_SIZE = <span class="hljs-number">512</span>       <span class="hljs-comment">#全连接层的节点个数</span>

<span class="hljs-comment">#定义CNN前向传播的过程</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inference</span>(<span class="hljs-params">input_tensor, train, regularizer</span>):</span>
	<span class="hljs-comment">#卷积层1</span>
    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;layer1-conv1&#x27;</span>):
        conv1_weights = tf.get_variable(
            <span class="hljs-string">&quot;weight&quot;</span>, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],
            initializer=tf.truncated_normal_initializer(stddev=<span class="hljs-number">0.1</span>))
        conv1_biases = tf.get_variable(<span class="hljs-string">&quot;bias&quot;</span>, [CONV1_DEEP], initializer=tf.constant_initializer(<span class="hljs-number">0.0</span>))
        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)
        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))

    <span class="hljs-comment">#池化层1</span>
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">&quot;layer2-pool1&quot;</span>):
        pool1 = tf.nn.max_pool(relu1, ksize = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],strides=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],padding=<span class="hljs-string">&quot;SAME&quot;</span>)

    <span class="hljs-comment">#卷积层2</span>
    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&quot;layer3-conv2&quot;</span>):
        conv2_weights = tf.get_variable(
            <span class="hljs-string">&quot;weight&quot;</span>, [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],
            initializer=tf.truncated_normal_initializer(stddev=<span class="hljs-number">0.1</span>))
        conv2_biases = tf.get_variable(<span class="hljs-string">&quot;bias&quot;</span>, [CONV2_DEEP], initializer=tf.constant_initializer(<span class="hljs-number">0.0</span>))
        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)
        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))

    <span class="hljs-comment">#池化层2</span>
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">&quot;layer4-pool2&quot;</span>):
        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&#x27;SAME&#x27;</span>)
        pool_shape = pool2.get_shape().as_list()
        nodes = pool_shape[<span class="hljs-number">1</span>] * pool_shape[<span class="hljs-number">2</span>] * pool_shape[<span class="hljs-number">3</span>]
        reshaped = tf.reshape(pool2, [pool_shape[<span class="hljs-number">0</span>], nodes])

    <span class="hljs-comment">#全连接层1</span>
    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;layer5-fc1&#x27;</span>):
        fc1_weights = tf.get_variable(<span class="hljs-string">&quot;weight&quot;</span>, [nodes, FC_SIZE],
                                      initializer=tf.truncated_normal_initializer(stddev=<span class="hljs-number">0.1</span>))
        <span class="hljs-keyword">if</span> regularizer != <span class="hljs-literal">None</span>: tf.add_to_collection(<span class="hljs-string">&#x27;losses&#x27;</span>, regularizer(fc1_weights))
        fc1_biases = tf.get_variable(<span class="hljs-string">&quot;bias&quot;</span>, [FC_SIZE], initializer=tf.constant_initializer(<span class="hljs-number">0.1</span>))

        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)
        <span class="hljs-keyword">if</span> train: fc1 = tf.nn.dropout(fc1, <span class="hljs-number">0.5</span>)
        
    <span class="hljs-comment">#全连接层2</span>
    <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">&#x27;layer6-fc2&#x27;</span>):
        fc2_weights = tf.get_variable(<span class="hljs-string">&quot;weight&quot;</span>, [FC_SIZE, NUM_LABELS],
                                      initializer=tf.truncated_normal_initializer(stddev=<span class="hljs-number">0.1</span>))
        <span class="hljs-keyword">if</span> regularizer != <span class="hljs-literal">None</span>: tf.add_to_collection(<span class="hljs-string">&#x27;losses&#x27;</span>, regularizer(fc2_weights))
        fc2_biases = tf.get_variable(<span class="hljs-string">&quot;bias&quot;</span>, [NUM_LABELS], initializer=tf.constant_initializer(<span class="hljs-number">0.1</span>))
        logit = tf.matmul(fc1, fc2_weights) + fc2_biases

    <span class="hljs-keyword">return</span> logit</code></pre></div>
<p><strong>定义训练流程</strong></p>
<div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -*-coding:utf-8-*-</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
<span class="hljs-keyword">import</span> LeNet5_infernece
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

BATCH_SIZE = <span class="hljs-number">100</span>
LEARNING_RATE_BASE = <span class="hljs-number">0.01</span>
LEARNING_RATE_DECAY = <span class="hljs-number">0.99</span>		<span class="hljs-comment">#学习率衰减率</span>
REGULARIZATION_RATE = <span class="hljs-number">0.0001</span>	<span class="hljs-comment">#正则化</span>
TRAINING_STEPS = <span class="hljs-number">1000</span>   		<span class="hljs-comment">#训练轮数</span>
MOVING_AVERAGE_DECAY = <span class="hljs-number">0.99</span>		<span class="hljs-comment">#滑动平均模型的指数衰减率</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">mnist</span>):</span>
    <span class="hljs-comment"># 定义输出为4维矩阵的placeholder</span>
    x = tf.placeholder(tf.float32, [
            BATCH_SIZE,
            LeNet5_infernece.IMAGE_SIZE,
            LeNet5_infernece.IMAGE_SIZE,
            LeNet5_infernece.NUM_CHANNELS],
        name=<span class="hljs-string">&#x27;x-input&#x27;</span>)
    y_ = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, LeNet5_infernece.OUTPUT_NODE], name=<span class="hljs-string">&#x27;y-input&#x27;</span>)
    
    <span class="hljs-comment">#这里的L2 regularizer是正则器，用于全连接层</span>
    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)
    <span class="hljs-comment">#这里的y是训练后的答案</span>
    y = LeNet5_infernece.inference(x,<span class="hljs-literal">False</span>,regularizer)
    <span class="hljs-comment">#定义滑动平均模型的迭代</span>
    global_step = tf.Variable(<span class="hljs-number">0</span>, trainable=<span class="hljs-literal">False</span>)

    <span class="hljs-comment"># 定义滑动平均类</span>
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
    <span class="hljs-comment"># 定义滑动平均操作</span>
    variables_averages_op = variable_averages.apply(tf.trainable_variables())
    <span class="hljs-comment"># 定义交叉熵函数</span>
    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="hljs-number">1</span>))
    <span class="hljs-comment"># 均值交叉熵函数</span>
    cross_entropy_mean = tf.reduce_mean(cross_entropy)
    <span class="hljs-comment"># 定义损失函数</span>
    loss = cross_entropy_mean + tf.add_n(tf.get_collection(<span class="hljs-string">&#x27;losses&#x27;</span>))
    <span class="hljs-comment"># 定义滑动平均下降的学习率</span>
    learning_rate = tf.train.exponential_decay(
        LEARNING_RATE_BASE,
        global_step,
        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,
        staircase=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># 定义训练迭代器</span>
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
    <span class="hljs-comment"># 定义训练操作</span>
    <span class="hljs-keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):
        train_op = tf.no_op(name=<span class="hljs-string">&#x27;train&#x27;</span>)
        

    <span class="hljs-comment"># 计算准确率</span>
    correct_prediction = tf.equal(tf.argmax(y, <span class="hljs-number">1</span>), tf.argmax(y_, <span class="hljs-number">1</span>))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    keep_prob = tf.placeholder(tf.float32)

    <span class="hljs-comment"># 初始化TensorFlow持久化类。</span>
    saver = tf.train.Saver()

    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
        tf.global_variables_initializer().run()
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(TRAINING_STEPS):
            xs, ys = mnist.train.next_batch(BATCH_SIZE)

            reshaped_xs = np.reshape(xs, (
                BATCH_SIZE,
                LeNet5_infernece.IMAGE_SIZE,
                LeNet5_infernece.IMAGE_SIZE,
                LeNet5_infernece.NUM_CHANNELS))
            
            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: reshaped_xs, y_: ys&#125;)

            <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                train_accuracy = accuracy.<span class="hljs-built_in">eval</span>(feed_dict=&#123;x: reshaped_xs, y_: ys, keep_prob: <span class="hljs-number">1.0</span>&#125;)
                <span class="hljs-comment">#print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (step, loss_value))</span>
                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The %d training accuracy is: %g&quot;</span> % (i,train_accuracy))


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span>():</span>
        <span class="hljs-keyword">pass</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>(<span class="hljs-params">argv=<span class="hljs-literal">None</span></span>):</span>
    mnist = input_data.read_data_sets(<span class="hljs-string">&quot;/Dataset/MNIST_data&quot;</span>, one_hot=<span class="hljs-literal">True</span>)
    train(mnist)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:
    main()
</code></pre></div>
<h3 id="3-MNIST手写数据集-GAN版"><a href="#3-MNIST手写数据集-GAN版" class="headerlink" title="3. MNIST手写数据集 - GAN版"></a>3. MNIST手写数据集 - GAN版</h3><p><strong>这里挂的是别人的代码，自己的代码一直出问题没解决。</strong></p>
<p><strong>生成器和判断器同样采用简单的三层全连接网络。</strong></p>
<div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf <span class="hljs-comment">#导入tensorflow</span>
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data <span class="hljs-comment">#导入手写数字数据集</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-comment">#导入numpy</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment">#plt是绘图工具，在训练过程中用于输出可视化结果</span>
<span class="hljs-keyword">import</span> matplotlib.gridspec <span class="hljs-keyword">as</span> gridspec <span class="hljs-comment">#gridspec是图片排列工具，在训练过程中用于输出可视化结果</span>
<span class="hljs-keyword">import</span> os <span class="hljs-comment">#导入os</span>
 

 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">xavier_init</span>(<span class="hljs-params">size</span>):</span> <span class="hljs-comment">#初始化参数时使用的xavier_init函数</span>
    in_dim = size[<span class="hljs-number">0</span>] 
    xavier_stddev = <span class="hljs-number">1.</span> / tf.sqrt(in_dim / <span class="hljs-number">2.</span>) <span class="hljs-comment">#初始化标准差</span>
    <span class="hljs-keyword">return</span> tf.random_normal(shape=size, stddev=xavier_stddev) <span class="hljs-comment">#返回初始化的结果</span>
 
X = tf.placeholder(tf.float32, shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">784</span>]) <span class="hljs-comment">#X表示真的样本(即真实的手写数字)</span>
 
D_W1 = tf.Variable(xavier_init([<span class="hljs-number">784</span>, <span class="hljs-number">128</span>])) <span class="hljs-comment">#表示使用xavier方式初始化的判别器的D_W1参数，是一个784行128列的矩阵</span>
D_b1 = tf.Variable(tf.zeros(shape=[<span class="hljs-number">128</span>])) <span class="hljs-comment">#表示全零方式初始化的判别器的D_1参数，是一个长度为128的向量</span>
 
D_W2 = tf.Variable(xavier_init([<span class="hljs-number">128</span>, <span class="hljs-number">1</span>])) <span class="hljs-comment">#表示使用xavier方式初始化的判别器的D_W2参数，是一个128行1列的矩阵</span>
D_b2 = tf.Variable(tf.zeros(shape=[<span class="hljs-number">1</span>])) <span class="hljs-comment">##表示全零方式初始化的判别器的D_1参数，是一个长度为1的向量</span>
 
theta_D = [D_W1, D_W2, D_b1, D_b2] <span class="hljs-comment">#theta_D表示判别器的可训练参数集合</span>
 
 
Z = tf.placeholder(tf.float32, shape=[<span class="hljs-literal">None</span>, <span class="hljs-number">100</span>]) <span class="hljs-comment">#Z表示生成器的输入(在这里是噪声)，是一个N列100行的矩阵</span>
 
G_W1 = tf.Variable(xavier_init([<span class="hljs-number">100</span>, <span class="hljs-number">128</span>])) <span class="hljs-comment">#表示使用xavier方式初始化的生成器的G_W1参数，是一个100行128列的矩阵</span>
G_b1 = tf.Variable(tf.zeros(shape=[<span class="hljs-number">128</span>])) <span class="hljs-comment">#表示全零方式初始化的生成器的G_b1参数，是一个长度为128的向量</span>
 
G_W2 = tf.Variable(xavier_init([<span class="hljs-number">128</span>, <span class="hljs-number">784</span>])) <span class="hljs-comment">#表示使用xavier方式初始化的生成器的G_W2参数，是一个128行784列的矩阵</span>
G_b2 = tf.Variable(tf.zeros(shape=[<span class="hljs-number">784</span>])) <span class="hljs-comment">#表示全零方式初始化的生成器的G_b2参数，是一个长度为784的向量</span>
 
theta_G = [G_W1, G_W2, G_b1, G_b2] <span class="hljs-comment">#theta_G表示生成器的可训练参数集合</span>
 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save</span>(<span class="hljs-params">saver, sess, logdir, step</span>):</span> <span class="hljs-comment">#保存模型的save函数</span>
   model_name = <span class="hljs-string">&#x27;model&#x27;</span> <span class="hljs-comment">#模型名前缀</span>
   checkpoint_path = os.path.join(logdir, model_name) <span class="hljs-comment">#保存路径</span>
   saver.save(sess, checkpoint_path, global_step=step) <span class="hljs-comment">#保存模型</span>
   <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The checkpoint has been created.&#x27;</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample_Z</span>(<span class="hljs-params">m, n</span>):</span> <span class="hljs-comment">#生成维度为[m, n]的随机噪声作为生成器G的输入</span>
    <span class="hljs-keyword">return</span> np.random.uniform(-<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, size=[m, n])
 
 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generator</span>(<span class="hljs-params">z</span>):</span> <span class="hljs-comment">#生成器，z的维度为[N, 100]</span>
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1) <span class="hljs-comment">#输入的随机噪声乘以G_W1矩阵加上偏置G_b1，G_h1维度为[N, 128]</span>
    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2 <span class="hljs-comment">#G_h1乘以G_W2矩阵加上偏置G_b2，G_log_prob维度为[N, 784]</span>
    G_prob = tf.nn.sigmoid(G_log_prob) <span class="hljs-comment">#G_log_prob经过一个sigmoid函数，G_prob维度为[N, 784]</span>
 
    <span class="hljs-keyword">return</span> G_prob <span class="hljs-comment">#返回G_prob</span>
 
 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discriminator</span>(<span class="hljs-params">x</span>):</span> <span class="hljs-comment">#判别器，x的维度为[N, 784]</span>
    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) <span class="hljs-comment">#输入乘以D_W1矩阵加上偏置D_b1，D_h1维度为[N, 128]</span>
    D_logit = tf.matmul(D_h1, D_W2) + D_b2 <span class="hljs-comment">#D_h1乘以D_W2矩阵加上偏置D_b2，D_logit维度为[N, 1]</span>
    D_prob = tf.nn.sigmoid(D_logit) <span class="hljs-comment">#D_logit经过一个sigmoid函数，D_prob维度为[N, 1]</span>
 
    <span class="hljs-keyword">return</span> D_prob, D_logit <span class="hljs-comment">#返回D_prob, D_logit</span>
 
 
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot</span>(<span class="hljs-params">samples</span>):</span> <span class="hljs-comment">#保存图片时使用的plot函数</span>
    fig = plt.figure(figsize=(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)) <span class="hljs-comment">#初始化一个4行4列包含16张子图像的图片</span>
    gs = gridspec.GridSpec(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>) <span class="hljs-comment">#调整子图的位置</span>
    gs.update(wspace=<span class="hljs-number">0.05</span>, hspace=<span class="hljs-number">0.05</span>) <span class="hljs-comment">#置子图间的间距</span>
 
    <span class="hljs-keyword">for</span> i, sample <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(samples): <span class="hljs-comment">#依次将16张子图填充进需要保存的图像</span>
        ax = plt.subplot(gs[i])
        plt.axis(<span class="hljs-string">&#x27;off&#x27;</span>)
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.set_aspect(<span class="hljs-string">&#x27;equal&#x27;</span>)
        plt.imshow(sample.reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">&#x27;Greys_r&#x27;</span>)
 
    <span class="hljs-keyword">return</span> fig
 
 
G_sample = generator(Z) <span class="hljs-comment">#取得生成器的生成结果</span>
D_real, D_logit_real = discriminator(X) <span class="hljs-comment">#取得判别器判别的真实手写数字的结果</span>
D_fake, D_logit_fake = discriminator(G_sample) <span class="hljs-comment">#取得判别器判别的生成的手写数字的结果</span>
 
D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real))) <span class="hljs-comment">#对判别器对真实样本的判别结果计算误差(将结果与1比较)</span>
D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake))) <span class="hljs-comment">#对判别器对虚假样本(即生成器生成的手写数字)的判别结果计算误差(将结果与0比较)</span>
D_loss = D_loss_real + D_loss_fake <span class="hljs-comment">#判别器的误差</span>
G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) <span class="hljs-comment">#生成器的误差(将判别器返回的对虚假样本的判别结果与1比较)</span>
 
dreal_loss_sum = tf.summary.scalar(<span class="hljs-string">&quot;dreal_loss&quot;</span>, D_loss_real) <span class="hljs-comment">#记录判别器判别真实样本的误差</span>
dfake_loss_sum = tf.summary.scalar(<span class="hljs-string">&quot;dfake_loss&quot;</span>, D_loss_fake) <span class="hljs-comment">#记录判别器判别虚假样本的误差</span>
d_loss_sum = tf.summary.scalar(<span class="hljs-string">&quot;d_loss&quot;</span>, D_loss) <span class="hljs-comment">#记录判别器的误差</span>
g_loss_sum = tf.summary.scalar(<span class="hljs-string">&quot;g_loss&quot;</span>, G_loss) <span class="hljs-comment">#记录生成器的误差</span>
 
summary_writer = tf.summary.FileWriter(<span class="hljs-string">&#x27;snapshots/&#x27;</span>, graph=tf.get_default_graph()) <span class="hljs-comment">#日志记录器</span>
 
D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D) <span class="hljs-comment">#判别器的训练器</span>
G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G) <span class="hljs-comment">#生成器的训练器</span>
 
mb_size = <span class="hljs-number">128</span> <span class="hljs-comment">#训练的batch_size</span>
Z_dim = <span class="hljs-number">100</span> <span class="hljs-comment">#生成器输入的随机噪声的列的维度</span>
 
mnist = input_data.read_data_sets(<span class="hljs-string">&#x27;../../MNIST_data&#x27;</span>, one_hot=<span class="hljs-literal">True</span>) <span class="hljs-comment">#mnist是手写数字数据集</span>
 
sess = tf.Session() <span class="hljs-comment">#会话层</span>
sess.run(tf.global_variables_initializer()) <span class="hljs-comment">#初始化所有可训练参数</span>
 
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;out/&#x27;</span>): <span class="hljs-comment">#初始化训练过程中的可视化结果的输出文件夹</span>
    os.makedirs(<span class="hljs-string">&#x27;out/&#x27;</span>)
 
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;snapshots/&#x27;</span>): <span class="hljs-comment">#初始化训练过程中的模型保存文件夹</span>
    os.makedirs(<span class="hljs-string">&#x27;snapshots/&#x27;</span>)
 
saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=<span class="hljs-number">50</span>) <span class="hljs-comment">#模型的保存器</span>
 
i = <span class="hljs-number">0</span> <span class="hljs-comment">#训练过程中保存的可视化结果的索引</span>
 
<span class="hljs-keyword">for</span> it <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100000</span>): <span class="hljs-comment">#训练100万次</span>
    <span class="hljs-keyword">if</span> it % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>: <span class="hljs-comment">#每训练1000次就保存一下结果</span>
        samples = sess.run(G_sample, feed_dict=&#123;Z: sample_Z(<span class="hljs-number">16</span>, Z_dim)&#125;)
 
        fig = plot(samples) <span class="hljs-comment">#通过plot函数生成可视化结果</span>
        plt.savefig(<span class="hljs-string">&#x27;out/&#123;&#125;.png&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">str</span>(i).zfill(<span class="hljs-number">3</span>)), bbox_inches=<span class="hljs-string">&#x27;tight&#x27;</span>) <span class="hljs-comment">#保存可视化结果</span>
        i += <span class="hljs-number">1</span>
        plt.close(fig)
 
    X_mb, _ = mnist.train.next_batch(mb_size) <span class="hljs-comment">#得到训练一个batch所需的真实手写数字(作为判别器的输入)</span>
 
    <span class="hljs-comment">#下面是得到训练一次的结果，通过sess来run出来</span>
    _, D_loss_curr, dreal_loss_sum_value, dfake_loss_sum_value, d_loss_sum_value = sess.run([D_solver, D_loss, dreal_loss_sum, dfake_loss_sum, d_loss_sum], feed_dict=&#123;X: X_mb, Z: sample_Z(mb_size, Z_dim)&#125;)
    _, G_loss_curr, g_loss_sum_value = sess.run([G_solver, G_loss, g_loss_sum], feed_dict=&#123;Z: sample_Z(mb_size, Z_dim)&#125;)
 
    <span class="hljs-keyword">if</span> it%<span class="hljs-number">100</span> ==<span class="hljs-number">0</span>: <span class="hljs-comment">#每过100次记录一下日志，可以通过tensorboard查看</span>
        summary_writer.add_summary(dreal_loss_sum_value, it)
        summary_writer.add_summary(dfake_loss_sum_value, it)
        summary_writer.add_summary(d_loss_sum_value, it)
        summary_writer.add_summary(g_loss_sum_value, it)
 
    <span class="hljs-keyword">if</span> it % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>: <span class="hljs-comment">#每训练1000次输出一下结果</span>
        save(saver, sess, <span class="hljs-string">&#x27;snapshots/&#x27;</span>, it)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Iter: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(it))
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;D loss: &#123;:.4&#125;&#x27;</span>. <span class="hljs-built_in">format</span>(D_loss_curr))
        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;G_loss: &#123;:.4&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(G_loss_curr))
        <span class="hljs-built_in">print</span>()
        

<span class="hljs-string">&#x27;&#x27;&#x27;</span>
<span class="hljs-string">--------------------- </span>
<span class="hljs-string">作者：jiongnima </span>
<span class="hljs-string">来源：CSDN </span>
<span class="hljs-string">原文：https://blog.csdn.net/jiongnima/article/details/80033169 </span>
<span class="hljs-string">&#x27;&#x27;&#x27;</span>
</code></pre></div>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/machine-learning/">machine learning</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/python/">python</a>
                    
                      <a class="hover-with-bg" href="/tags/tensorflow/">tensorflow</a>
                    
                      <a class="hover-with-bg" href="/tags/machinelearning/">machinelearning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2018/08/08/%E6%B5%85%E8%B0%88%E6%B3%A8%E5%85%A5%E6%94%BB%E5%87%BB%E5%8F%8A%E5%B0%8F%E7%BB%93/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">浅谈注入攻击及小结</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2017/10/25/%E4%BD%BF%E7%94%A8Hexo%E5%92%8CGithub%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">
                        <span class="hidden-mobile">使用Hexo和Github快速搭建个人博客</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>










  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
